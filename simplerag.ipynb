{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T06:27:53.877502Z",
     "start_time": "2026-02-08T06:27:53.203921Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "text_document=loader.load()\n",
    "text_document"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The most famous speech in history is widely considered to be \"I Have a Dream\" by Martin Luther King Jr., delivered on August 28, 1963, during the March on Washington for Jobs and Freedom.  This powerful address, calling for an end to racism and equality for all Americans, remains a defining moment in the Civil Rights Movement and continues to inspire global efforts for justice and human rights. It is consistently ranked at the top of lists such as Top 100 American Speeches of the 20th Century by the University of Wisconsin-Madison and Texas A&M University, and is recognized as the most influential and enduring speech in American history.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:27:53.954670Z",
     "start_time": "2026-02-08T06:27:53.944912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ],
   "id": "7e49f0c1ab580399",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:27:54.156427Z",
     "start_time": "2026-02-08T06:27:53.997055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ],
   "id": "92043f40ff27bf28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:27:54.942845Z",
     "start_time": "2026-02-08T06:27:54.175406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\n",
    "        \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\n",
    "    ],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=SoupStrainer(\n",
    "            class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])\n"
   ],
   "id": "441cbe0c506be02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Extrinsic Hallucinations in LLMs\n",
      "    \n",
      "Date: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:27:54.974296Z",
     "start_time": "2026-02-08T06:27:54.964810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "documents[:5]\n"
   ],
   "id": "594063f8d095a512",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model\u00e2\u20ac\u2122s tendency to hallucinate.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='Given a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='Knowledge categorization of close-book QA examples based on how likely the model outputs correct answers. (Image source: Gekhman et al. 2024)\\n\\nSome interesting observations of the experiments, where dev set accuracy is considered a proxy for hallucinations.\\n\\nUnknown examples are fitted substantially slower than Known.\\nThe best dev performance is obtained when the LLM fits the majority of the Known training examples but only a few of the Unknown ones. The model starts to hallucinate when it learns most of the Unknown examples.\\nAmong Known examples, MaybeKnown cases result in better overall performance, more essential than HighlyKnown ones.\\n\\n\\n\\nTrain and dev performance over time when fine-tuning on half `Known` and half `Unknown` examples. `Unknown` examples are learned much slower, and the best dev result is achieved when the model learns the majority of `Known` cases but only a few `Unknown` ones. (Image source: Gekhman et al. 2024)\\n\\nThese empirical results from Gekhman et al. (2024) point out the risk of using supervised fine-tuning for updating LLMs\u00e2\u20ac\u2122 knowledge.\\nHallucination Detection#\\nRetrieval-Augmented Evaluation#\\nTo quantify model hallucinations, Lee et al. (2022) introduced a new benchmark dataset, FactualityPrompt, consisting of both factual and nonfactual prompts. This dataset uses Wikipedia documents or sentences as the knowledge base for factuality grounding. The Wikipedia documents are known ground-truth from the FEVER dataset, and the sentences are selected based on tf-idf or sentence embedding-based similarity.\\n\\n\\nThe evaluation framework for the FactualityPrompt benchmark.(Image source: Lee, et al. 2022)\\n\\nGiven the model continuation and paired Wikipedia text, two evaluation metrics for hallucination are considered:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='Hallucination NE (Named Entity) errors: Using a pretrained entity detection model and document-level grounding, this metric measures the fraction of detected named entities that do not appear in the ground truth document.\\nEntailment ratios: Using a RoBERTa model fine-tuned on MNLI and sentence-level knowledge grounding, this metric calculates the fraction of generated sentences that are marked as relevant to the paired Wikipedia sentence by the entailment model.\\n\\nLower NE errors and higher entailment ratios indicate higher factuality, and both metrics are found to be correlated with human annotations. Larger models are found to perform better on this benchmark.\\nFActScore (Factual precision in Atomicity Score; Min et al. 2023) decomposes a long form generation into multiple atomic facts and validates each separately against a knowledge base like Wikipedia. Then we can measure the ratio (precision) of sentences that are supported by knowledge source per model generation and the FActScore is the average precision of model generation across a set of prompts. The paper experimented with several ways of factuality validation on the task of people\u00e2\u20ac\u2122s biographies generation and found that using retrieval is consistent better than non-context LLM. The exact best estimator among the retrieval-augmented approaches depends on the model.\\n\\nNon-context LLM: Prompt LLM directly with <atomic-fact> True or False? without additional context.\\nRetrieval\u00e2\u2020\u2019LLM: Prompt with $k$ related passages retrieved from the knowledge source as context.\\nNonparametric probability (NP)): Compute the average likelihood of tokens in the atomic fact by a masked LM and use that to make a prediction.\\nRetrieval\u00e2\u2020\u2019LLM + NP: Ensemble of two methods.\\n\\nSome interesting observations on model hallucination behavior:')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:28:13.757933Z",
     "start_time": "2026-02-08T06:27:54.998267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings",
    "from langchain_community.vectorstores import FAISS",
    "",
    "embeddings = OllamaEmbeddings(",
    "    model=\"nomic-embed-text\"",
    ")",
    "",
    "db = FAISS.from_documents(",
    "    documents[:20],",
    "    embedding=embeddings",
    ")",
    "",
    "retriever=db.as_retriever()",
    "retriever",
    ""
   ],
   "id": "edb6a66319ef683d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:28:13.860585Z",
     "start_time": "2026-02-08T06:28:13.776847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query=\"What is hallucination\"\n",
    "result=db.similarity_search(query)\n",
    "result[1].page_content"
   ],
   "id": "bf4addb9d6e67b3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model\u00e2\u20ac\u2122s tendency to hallucinate.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:30:35.660774Z",
     "start_time": "2026-02-08T06:30:35.655372Z"
    }
   },
   "cell_type": "code",
   "source": "db",
   "id": "9c08acb7c196d08a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2762f57a250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:32:47.877969Z",
     "start_time": "2026-02-08T06:32:47.873845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ],
   "id": "a0f126e8a9b7ad53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:40:12.100390Z",
     "start_time": "2026-02-08T06:40:12.092977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template('''\n",
    "Answer the following question based only on the providing detailed answer.\n",
    "i will tip you $1000 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question:{input}''')"
   ],
   "id": "d8c506a457a47068",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T06:49:26.881060Z",
     "start_time": "2026-02-08T06:49:26.856257Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_classic.chains.combine_documents import create_stuff_documents_chain",
   "id": "e47ea08f71683304",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c843000896d026e"
  },
  {
   "cell_type": "code",
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}